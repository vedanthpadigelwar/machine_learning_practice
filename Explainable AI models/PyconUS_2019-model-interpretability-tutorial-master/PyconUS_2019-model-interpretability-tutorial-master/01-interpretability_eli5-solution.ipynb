{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Model Interpretability: Eli5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note: This notebook uses features introduced in Python 3.6 and sklearn 0.20. \n", "\n", "First we'll need a few imports:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Obviously\n", "import pandas as pd\n", "\n", "# Some sklearn tools for preprocessing. \n", "# ColumnTransformer was introduced in 0.20 so make sure you have this version\n", "from sklearn.preprocessing import OneHotEncoder\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.model_selection import train_test_split, GridSearchCV\n", "\n", "# In terms of metric we'll need balanced accuracy score and classification report\n", "# More on that later...\n", "from sklearn.metrics import balanced_accuracy_score, classification_report\n", "\n", "# Our algorithms, from the easiest to the hardest to intepret.\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from lightgbm.sklearn import LGBMClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The data is based on a dataset available [here](https://archive.ics.uci.edu/ml/datasets/bank+marketing). \n", "\n", "It consists of data from marketing campaigns of a Portuguese bank. We will build classifiers that given characteristics about an individual, and how they were approached by the marketing team, we can predict whether they will subscribe to a term deposit (column `y`). \n", "\n", "We will build multiple models with algorithms of different complexity. We'll keep the modelling part of the job simple though, so we can focus on interpreting our models and see what recommendation we can make to the domain experts (here the person in charge of the marketing campaign)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"data/bank.csv\")\n", "\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["df.y.value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The dataset is imbalanced, we will need to keep that in mind when building our models..."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get the feature matrix (X) and the target vector (y)\n", "\n", "y = df[\"y\"].map({\"no\": 0, \"yes\": 1})\n", "X = df.drop(\"y\", axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at the features in the X matrix and their types:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X.dtypes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["1. `age` (numeric)\n", "2. `job` : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n", "3. `marital` : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n", "4. `education` (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n", "5. `default`: has credit in default? (categorical: 'no','yes','unknown')\n", "6. `housing`: has housing loan? (categorical: 'no','yes','unknown')\n", "7. `loan`: has personal loan? (categorical: 'no','yes','unknown')\n", "8. `contact`: contact communication type (categorical: 'cellular','telephone') \n", "9. `month`: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n", "10. `day_of_week`: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n", "11. `campaign`: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n", "12. `pdays`: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n", "13. `previous`: number of contacts performed before this campaign and for this client (numeric)\n", "14. `poutcome`: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What features do you expect to be relevant to predict whether someone will subscribe to our plan?\n", "\n", "To find those, we will train models and interpret them."]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, let's create a list of numerical and categorical features so we can easily select one or the other later:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Some such as default would be binary features, but since\n", "# they have a third class \"unknown\" we'll process them as non binary categorical\n", "num_features = [\"age\", \"campaign\", \"pdays\", \"previous\"]\n", "\n", "cat_features = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\",\n", "                \"contact\", \"month\", \"day_of_week\", \"poutcome\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, we'll define a new `ColumnTransformer` object that just keeps our numerical features as they are and apply one hot encoding on our categorical features. This transformer can be fitted on the training set and used to transform the test set in the same way:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["preprocessor = ColumnTransformer([(\"numerical\", \"passthrough\", num_features), \n", "                                  (\"categorical\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n", "                                   cat_features)])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's split the data into training and test sets. Create `X_train`, `X_test`, `y_train` and `y_test` using `train_test_split` from sklearn:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.3, random_state=42)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can create processed versions of X_train and X_test, ready to be used by our algorithms. To do so we will start by fitting the processor on X_train:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["preprocessor.fit(X_train)\n", "\n", "preprocessor.transform(X_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When we call transform, the preprocessor returns a numpy array, which is great for Machine Learning algorithms to process ... but not so great for us humans to interpret. So we will wrap it up back to DataFrame with nice column names. To do so we will need to give nice names to the dummy feature generated by the one hot encoder. The code below extracts the list of categories, creates nice names for the dummy feature and create a new list `all_features` with good names for our columns:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get the list of categories generated by the process\n", "ohe_categories = preprocessor.named_transformers_[\"categorical\"].categories_\n", "\n", "# Create nice names for our one hot encoded features\n", "new_ohe_features = [f\"{col}__{val}\" for col, vals in zip(cat_features, ohe_categories) for val in vals]\n", "\n", "# Create a new list with all names of features\n", "all_features = num_features + new_ohe_features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, now we can create our preprocessed DataFrames with good column names:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = pd.DataFrame(preprocessor.transform(X_train), columns=all_features)\n", "X_test = pd.DataFrame(preprocessor.transform(X_test), columns=all_features)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks good!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can define the 4 models we will interpret in this tutorial:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's start with a LogisticRegression:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Logistic Regression\n", "lr_model = LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we keep most parameters to default as we'll tune the model later. But defined `class_weight` to be balanced in order to take into account the imbalance in our dataset. \n", "\n", "Create `dt_model`, `rf_model` and `lgb_model` respectively using a Decistion Tree, a Random Forest and an LightGBM classifier, everytime make sure you set `class_weight` to `\"balanced\"`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Decision Tree\n", "dt_model = DecisionTreeClassifier(class_weight=\"balanced\")\n", "\n", "# Random Forest\n", "rf_model = RandomForestClassifier(class_weight=\"balanced\", n_estimators=150, n_jobs=-1)\n", "\n", "# LightGBM -- Boosting model\n", "lgb_model = LGBMClassifier(class_weight=\"balanced\", n_jobs=-1)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using Eli5 to get importance of features globally and locally"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### With Logistic Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First let's fine tune our logistic regression and evaluate its performance. Here we'll use [\"balanced_accuracy\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) as metric to optimise so it takes into account the imbalance of classes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gs = GridSearchCV(lr_model, {\"C\": [1., 1.3, 1.5]}, n_jobs=-1, cv=5, scoring=\"balanced_accuracy\")\n", "gs.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's see our best parameters and score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the best parameters and best score from the GridSearch object:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(gs.best_params_)\n", "print(gs.best_score_)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since `GridSearchCV` has a parameter `refit` that is `True` by default, it has re-fitted the model with best parameters on the whole training set. We can access it using `best_estimator_`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# LR model is our best trained model for LR\n", "lr_model = gs.best_estimator_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check that our model has properly been updated:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_model.get_params()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Generate predictions on the test set; `y_pred`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = lr_model.predict(X_test)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute balanced accuracy:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["balanced_accuracy_score(y_test, y_pred)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the classification report:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(classification_report(y_test, y_pred))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, so now we have a simple model that can predict if someone will subscribe to our plan. This isn't a great model in terms of accuracy, but it's simple enough that we can easily look inside the box and understand how it works.\n", "\n", "For that we'll use `eli5` that will allow us to visualise the weights associated to each feature.\n", "\n", "Import eli5 and use `show_weights` to visualise the weights of your model, don't forget to pass your column names as well so `eli5` can display them instead of the column number (check the documentation for `show_weights`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import eli5\n", "\n", "eli5.show_weights(lr_model, feature_names=all_features)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That's easier to read. This table gives us the weight associated to each feature. The amplitude tells us how much of an impact a feature has on the predictions on average, the sign tells us in which direction. Here if the campaign is in March, it increases the probability of the prospect to subscribe to the plan significantly. Has the marketting team done something different in March? Or are prospects just more likely to subscribe in March? That's a question to ask to the marketting team, depending on the answer, this finding might or might not be useful."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also use `eli5` to explain a specific prediction, let's pick a row in the test data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["i = 4\n", "X_test.iloc[[i]]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_test.iloc[i]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our prospect subsribed to the term deposit after the campaign! Let's see what our model would have predicted and how we could explain it to the domain expert."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eli5.show_prediction(lr_model, X_test.iloc[i],\n", "                     feature_names=all_features, show_feature_values=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here `eli5` does something really simple: knowing the weights associated to each feature and the actual value for all features for this specific observation, it can compute the contribution of each feature towards the prediction. \n", "\n", "For this prediction, it looks like the most important factor was that the prospect was contacted via phone (`contact__cellular`==1) and did not have a default (`default__no`==1). That's interesting information that you could share with the team member who was in touch with this prospect, to trigger a discussion on why those features were important."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### with a Decision Tree"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`eli5` can also be used to intepret decision trees:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use grid search to find the best parameters for your Decision Tree:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gs = GridSearchCV(dt_model, {\"max_depth\": [8, 10, 15], \n", "                             \"min_samples_split\": [.005, .01, .05]}, \n", "                  n_jobs=-1, cv=5, scoring=\"balanced_accuracy\")\n", "\n", "gs.fit(X_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's see our best parameters and score:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(gs.best_params_)\n", "print(gs.best_score_)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Store the best model `best_estimator_` into a new variable `dt_model` that we can try to interpret later:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dt_model = gs.best_estimator_\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check accuracy and classification report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = dt_model.predict(X_test)\n", "\n", "print(balanced_accuracy_score(y_test, y_pred))\n", "print(classification_report(y_test, y_pred))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unfortunately, for Decision Trees, `eli5` only gives the simple feature importance. It tells you how important different features are on average relative to each other but not how they impact the outcome.\n", "\n", "Use `show_weights` on your dt_model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["eli5.show_weights(dt_model, feature_names=all_features)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this model, the most important feature seems to be `pdays` but we don't know if the more days the more likely it is that someone will subscribe, or the opposite. It can be useful to debug your model and know if it seems to pick up something that it shouldn't, but appart for that, it isn't too useful: you won't be able to properly explain what your model does to someone thanks to that."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Limitations of the method...\n", "\n", "So, `eli5`'s `show_weights` method is good, but for more complex models, such as trees the information provided starts to be less helpful. Since `show_weights` is accessing the internal weights of a model, it does not work with all algorithms, making it harder to compare different models you might have built.\n", "\n", "`eli5` implements another technique called Permutation Importance that is model agnostic and works for any black box model. By shuffling at random the values of a feature, we can observe how that affects the predictions and quantify how important that feature is. If we repeat on all features, we can get the overall importance of each feature and compare them. Let's try to do that on our models."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Using Permutation Importance for global and model agnostic feature importance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from eli5.sklearn import PermutationImportance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First will need to create a new instance of `PermutationImportance` that takes our trained model to be interpreted and the scoring method (`\"balanced_accuracy\"`) we which to use to quantify the importance of our features.\n", "\n", "Create a PermutationImportance object to interpret your Decision Tree model, and call it `perm`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["perm = PermutationImportance(dt_model, scoring=\"balanced_accuracy\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the next step, we will need to `fit` our Permutation Importance. We could do so using the training set, but that would just tell us how important each feature was when trying to minimise the accuracy, not how your model will work on new data. \n", "\n", "To ensure your interpretation corresponds to how your model works when it is used on new data, you need to train the permutation importance on unseen data, using the test set for instance.\n", "\n", "Call `fit` on your Permutation Importance object, then use `eli5`'s `show_weigths` as before, but passing the `perm` object instead of the model itself. This will plot your new feature importance:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["perm.fit(X_test, y_test)\n", "eli5.show_weights(perm, feature_names=all_features)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again, here the feature importance is only given as an amplitude, we do not know in what direction it impacts the outcome. But the interpretation is quite interesting; when you will use your model on new data, to predict whether someone will subscribe or not to your plan, the most important thing it will need to get the prediction right is whether you contacted the person by telephone. You're not looking at what the model gave the most importance too whilst learning, but how it will give importance to features from now on based on what it has learnt. It also tells you that variables like whether the person got divorced will be pretty much irrelevant when your model is used.\n", "\n", "Since Permutation Importance is model agnostic, we can use it on any other model that we have built, and compare the feature importance. Let's do that with a Random Forest and LightGBM model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### with a Random Forest"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use grid search to find the best parameters for your `rf_model`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gs = GridSearchCV(rf_model, {\"max_depth\": [10, 15, 20],\n", "                             \"min_samples_split\": [.005, .01, .05]}, \n", "                  n_jobs=-1, cv=5, scoring=\"balanced_accuracy\")\n", "\n", "gs.fit(X_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check your best params and score:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(gs.best_params_)\n", "print(gs.best_score_)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create `rf_model`, your model with best parameters:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["rf_model = gs.best_estimator_\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Generate predictions and check accuracy and classification report:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = rf_model.predict(X_test)\n", "print(balanced_accuracy_score(y_test, y_pred))\n", "print(classification_report(y_test, y_pred))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build a new Permutation Importance object for your `rf_model` and fit it on the test data. What are the most important features here?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["perm = PermutationImportance(rf_model, scoring=\"balanced_accuracy\")\n", "perm.fit(X_test, y_test)\n", "\n", "eli5.show_weights(perm, feature_names=all_features)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We find again that the feature that tells if prospects were contacted by telephone will have a big impact on whether our model will be right or wrong on new data. This seem to be particularly relevant, that's probably something we want to report to the marketing team."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's train our LightGBM model as well"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now find the best parameters for your pipeline, and create `lgb_model` with the best parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gs = GridSearchCV(lgb_model, {\"max_depth\": [8, 10, 12],\n", "                              \"min_child_samples\": [20, 40, 60],\n", "                              \"n_estimators\": [25, 50, 75]},\n", "                  n_jobs=-1, cv=5, scoring=\"balanced_accuracy\")\n", "\n", "gs.fit(X_train, y_train)\n", "\n", "print(gs.best_params_)\n", "print(gs.best_score_)\n", "lgb_model = gs.best_estimator_\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Generate predictions and check the accuracy and classification report:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = lgb_model.predict(X_test)\n", "balanced_accuracy_score(y_test, y_pred)\n", "print(classification_report(y_test, y_pred))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build a new Permutation Importance object for your `lgb_model` and fit it on the test data. What are the most important features here?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["perm = PermutationImportance(lgb_model, scoring=\"balanced_accuracy\")\n", "perm.fit(X_test, y_test)\n", "\n", "eli5.show_weights(perm, feature_names=all_features)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Quite similar features again, it looks like all our models, even the most complex ones seem to agree on what is important in the data.\n", "\n", "Unfortunately, using Permutation Importance, we aren't able to tell much more than that. We cannot tell in what direction this will impact the outcome (which would be useful to make recommendations to the marketing team), nor can we explain a specific prediction... But at least this is model agnostic, so we can use this method for any model and compare the results as we did.\n", "\n", "The next part of the tutorial is dedicated to LIME, a method that is also model agnostic and allows us to generate explainations of specific, local predictions."]}]}